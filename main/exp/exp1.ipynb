{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗一\n",
    "\n",
    "- 使用 `Taylor Videos` 的 video representation 對兒童肢體診斷影片進行去識別化處理，以避免隱私洩漏。\n",
    "- 使用 `Taylor Videos` 的 action recognition model 進行肢體診斷影片的動作識別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylorvideo(video_path, terms, window_size, step_size):\n",
    "    \n",
    "    if window_size - 3 < terms:\n",
    "        print(\"The given temporal block length is not enough to compute K terms.\")\n",
    "        \n",
    "    else:\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "        vlen = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(\"Video length: %d\" % vlen, \" |  FPS: %d\" % fps)\n",
    "        \n",
    "        success, image = vidcap.read()\n",
    "        count = 1\n",
    "        while success:\n",
    "            \n",
    "            if count < window_size:\n",
    "                success,image = vidcap.read()\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    # Replace negative values with 0\n",
    "    tensor[tensor < 0] = 0\n",
    "    # Scale the values to fit in the range [0, 255]\n",
    "    max_val = tensor.max()\n",
    "    if max_val > 0:\n",
    "        tensor = (tensor * 255 / max_val).to(torch.uint8)\n",
    "    else:\n",
    "        tensor = tensor.to(torch.uint8)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoConvert(vid_path,o,terms,tPrime):\n",
    "    ts = time.time()\n",
    "    if (tPrime <= terms+3):\n",
    "        tPrime = terms + 3\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    fpsT = cap.get(cv2.CAP_PROP_FPS)\n",
    "    vidlength = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"length = \", vidlength)\n",
    "    print(\"fps = \", fpsT)\n",
    "    print(f\"Check point 1: {time.time()-ts}\")\n",
    "    ret,frame = cap.read()\n",
    "    norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "    norm_g = torch.div(norm_g, 255.0)\n",
    "    h, w = norm_g.shape\n",
    "    print(f\"Check point 2: {time.time()-ts}\")\n",
    "    \n",
    "    length = terms + 3\n",
    "    full_difference_list = torch.zeros((length,length,h,w), dtype=torch.float64)\n",
    "    full_difference_list[0,0,:,:] = norm_g\n",
    "\n",
    "    for initialInc in range(1,terms+3):\n",
    "        ret, frame = cap.read()\n",
    "        norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "        norm_g = torch.div(norm_g, 255.0)\n",
    "        full_difference_list[0,initialInc,:,:] = norm_g\n",
    "    print(f\"Check point 3: {time.time()-ts}\")\n",
    "\n",
    "    img = torch.zeros(((vidlength-tPrime+1),h,w,3), dtype=torch.uint8)\n",
    "    \n",
    "    cp31 = []\n",
    "    cp32 = []\n",
    "    cp33 = []\n",
    "    pbar = tqdm(range(0,vidlength-tPrime+1))\n",
    "    for sequences in pbar:\n",
    "        local_ts = time.time()\n",
    "        if sequences == 0:\n",
    "            for listInc in range(1,terms+3):\n",
    "                full_difference_list[listInc,:,:,:] = torch.nn.functional.pad((full_difference_list[listInc-1,1:,:,:]-full_difference_list[listInc-1,:-1,:,:]),(0,0,0,0,0,1))\n",
    "        if sequences != 0:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            full_difference_list = torch.roll(full_difference_list, -1, 1)\n",
    "            \n",
    "            norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "            norm_g = torch.div(norm_g, 255.0)\n",
    "            full_difference_list[0,length-1,:,:] = norm_g\n",
    "            for listInc in range(1,terms+3):\n",
    "                full_difference_list[listInc,length-1-listInc,:,:] = full_difference_list[listInc-1,length-listInc,:,:] - full_difference_list[listInc-1,length-1-listInc,:,:]\n",
    "        \n",
    "        cp31.append(time.time()-local_ts)\n",
    "\n",
    "        t1Sum = 0\n",
    "        t2Sum = 0\n",
    "        t3Sum = 0\n",
    "\n",
    "        dummy = full_difference_list[0,0,:,:].unsqueeze(0).repeat(length, 1, 1)\n",
    "        xa_Tensor =  full_difference_list[0,:,:,:] - dummy\n",
    "\n",
    "        for incB in range(0,terms):\n",
    "            part = torch.div(torch.pow(xa_Tensor, incB), math.factorial(int(incB)))\n",
    "            t1Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+1,0,:,:])\n",
    "            t2Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+2,0,:,:])\n",
    "            t3Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+3,0,:,:])\n",
    "\n",
    "        cp32.append(time.time()-local_ts)\n",
    "\n",
    "        # R channel\n",
    "        t1Sum = preprocess_tensor(t1Sum / tPrime)\n",
    "        img[sequences,:,:,0] = t1Sum \n",
    "        # G channel\n",
    "        t2Sum = preprocess_tensor(t2Sum / tPrime)\n",
    "        img[sequences,:,:,1] = t2Sum \n",
    "        # B channel\n",
    "        t3Sum = preprocess_tensor(t3Sum / tPrime)\n",
    "        img[sequences,:,:,2] = t3Sum\n",
    "\n",
    "        cp33.append(time.time()-local_ts)\n",
    "\n",
    "        pbar.set_postfix({'cp31': np.mean(cp31), 'cp32': np.mean(cp32), 'cp33': np.mean(cp33)})\n",
    "        \n",
    "    print(f\"Check point 4: {time.time()-ts}\")\n",
    "    cap.release()\n",
    "    print(type(img))\n",
    "    print(img.shape)\n",
    "    print(img.dtype)\n",
    "    fpsT = int(fpsT)\n",
    "    print(type(fpsT))\n",
    "    torchvision.io.write_video(filename=o, video_array=img, fps=fpsT, video_codec='mpeg4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length =  116\n",
      "fps =  24.0\n",
      "Check point 1: 0.017267465591430664\n",
      "Check point 2: 0.05207180976867676\n",
      "Check point 3: 0.10076522827148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:12<00:00,  9.18it/s, cp31=0.0335, cp32=0.0963, cp33=0.107]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check point 4: 12.217063903808594\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([111, 720, 1280, 3])\n",
      "torch.uint8\n",
      "<class 'float'>\n",
      "<class 'numpy.float64'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'numerator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# videoConvert(\"test.mp4\",\"test-taylor.mp4\",3,0)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvideoConvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./brush.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./brush-taylor.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 87\u001b[0m, in \u001b[0;36mvideoConvert\u001b[1;34m(vid_path, o, terms, tPrime)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(img\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(fpsT))\n\u001b[1;32m---> 87\u001b[0m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfpsT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_codec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmpeg4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\io\\video.py:100\u001b[0m, in \u001b[0;36mwrite_video\u001b[1;34m(filename, video_array, fps, video_codec, options, audio_array, audio_fps, audio_codec, audio_options)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(fps))\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m av\u001b[38;5;241m.\u001b[39mopen(filename, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m container:\n\u001b[1;32m--> 100\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[43mcontainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_codec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     stream\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m=\u001b[39m video_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m    102\u001b[0m     stream\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m=\u001b[39m video_array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mav\\\\container\\\\output.pyx:93\u001b[0m, in \u001b[0;36mav.container.output.OutputContainer.add_stream\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mav\\\\utils.pyx:51\u001b[0m, in \u001b[0;36mav.utils.to_avrational\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'numerator'"
     ]
    }
   ],
   "source": [
    "# videoConvert(\"test.mp4\",\"test-taylor.mp4\",3,0)\n",
    "videoConvert(\"./brush.mp4\",\"./brush-taylor.mp4\",3,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
