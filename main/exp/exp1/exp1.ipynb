{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 實驗一\n",
    "\n",
    "- 使用 `Taylor Videos` 的 video representation 對兒童肢體診斷影片進行去識別化處理，以避免隱私洩漏。\n",
    "- 使用 `Taylor Videos` 的 action recognition model 進行肢體診斷影片的動作識別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import glob\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def taylorvideo(video_path, terms, window_size, step_size):\n",
    "    \n",
    "    if window_size - 3 < terms:\n",
    "        print(\"The given temporal block length is not enough to compute K terms.\")\n",
    "        \n",
    "    else:\n",
    "        vidcap = cv2.VideoCapture(video_path)\n",
    "        \n",
    "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "        vlen = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(\"Video length: %d\" % vlen, \" |  FPS: %d\" % fps)\n",
    "        \n",
    "        success, image = vidcap.read()\n",
    "        count = 1\n",
    "        while success:\n",
    "            \n",
    "            if count < window_size:\n",
    "                success,image = vidcap.read()\n",
    "                count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tensor(tensor):\n",
    "    # Replace negative values with 0\n",
    "    tensor[tensor < 0] = 0\n",
    "    # Scale the values to fit in the range [0, 255]\n",
    "    max_val = tensor.max()\n",
    "    if max_val > 0:\n",
    "        tensor = (tensor * 255 / max_val).to(torch.uint8)\n",
    "    else:\n",
    "        tensor = tensor.to(torch.uint8)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoConvert(vid_path,o,terms,tPrime):\n",
    "    ts = time.time()\n",
    "    if (tPrime <= terms+3):\n",
    "        tPrime = terms + 3\n",
    "    cap = cv2.VideoCapture(vid_path)\n",
    "    fpsT = cap.get(cv2.CAP_PROP_FPS)\n",
    "    vidlength = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"length = \", vidlength)\n",
    "    print(\"fps = \", fpsT)\n",
    "    print(f\"Check point 1: {time.time()-ts}\")\n",
    "    ret,frame = cap.read()\n",
    "    norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "    norm_g = torch.div(norm_g, 255.0)\n",
    "    h, w = norm_g.shape\n",
    "    print(f\"Check point 2: {time.time()-ts}\")\n",
    "    \n",
    "    length = terms + 3\n",
    "    full_difference_list = torch.zeros((length,length,h,w), dtype=torch.float64)\n",
    "    full_difference_list[0,0,:,:] = norm_g\n",
    "\n",
    "    for initialInc in range(1,terms+3):\n",
    "        ret, frame = cap.read()\n",
    "        norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "        norm_g = torch.div(norm_g, 255.0)\n",
    "        full_difference_list[0,initialInc,:,:] = norm_g\n",
    "    print(f\"Check point 3: {time.time()-ts}\")\n",
    "\n",
    "    img = torch.zeros(((vidlength-tPrime+1),h,w,3), dtype=torch.uint8)\n",
    "    \n",
    "    cp31 = []\n",
    "    cp32 = []\n",
    "    cp33 = []\n",
    "    pbar = tqdm(range(0,vidlength-tPrime+1))\n",
    "    for sequences in pbar:\n",
    "        local_ts = time.time()\n",
    "        if sequences == 0:\n",
    "            for listInc in range(1,terms+3):\n",
    "                full_difference_list[listInc,:,:,:] = torch.nn.functional.pad((full_difference_list[listInc-1,1:,:,:]-full_difference_list[listInc-1,:-1,:,:]),(0,0,0,0,0,1))\n",
    "        if sequences != 0:\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            full_difference_list = torch.roll(full_difference_list, -1, 1)\n",
    "            \n",
    "            norm_g = torch.from_numpy(cv2.cvtColor(frame,  cv2.COLOR_BGR2GRAY))\n",
    "            norm_g = torch.div(norm_g, 255.0)\n",
    "            full_difference_list[0,length-1,:,:] = norm_g\n",
    "            for listInc in range(1,terms+3):\n",
    "                full_difference_list[listInc,length-1-listInc,:,:] = full_difference_list[listInc-1,length-listInc,:,:] - full_difference_list[listInc-1,length-1-listInc,:,:]\n",
    "        \n",
    "        cp31.append(time.time()-local_ts)\n",
    "\n",
    "        t1Sum = 0\n",
    "        t2Sum = 0\n",
    "        t3Sum = 0\n",
    "\n",
    "        dummy = full_difference_list[0,0,:,:].unsqueeze(0).repeat(length, 1, 1)\n",
    "        xa_Tensor =  full_difference_list[0,:,:,:] - dummy\n",
    "\n",
    "        for incB in range(0,terms):\n",
    "            part = torch.div(torch.pow(xa_Tensor, incB), math.factorial(int(incB)))\n",
    "            t1Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+1,0,:,:])\n",
    "            t2Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+2,0,:,:])\n",
    "            t3Sum += torch.mul(torch.sum(part,0),full_difference_list[incB+3,0,:,:])\n",
    "\n",
    "        cp32.append(time.time()-local_ts)\n",
    "\n",
    "        # R channel\n",
    "        t1Sum = preprocess_tensor(t1Sum / tPrime)\n",
    "        img[sequences,:,:,0] = t1Sum \n",
    "        # G channel\n",
    "        t2Sum = preprocess_tensor(t2Sum / tPrime)\n",
    "        img[sequences,:,:,1] = t2Sum \n",
    "        # B channel\n",
    "        t3Sum = preprocess_tensor(t3Sum / tPrime)\n",
    "        img[sequences,:,:,2] = t3Sum\n",
    "\n",
    "        cp33.append(time.time()-local_ts)\n",
    "\n",
    "        pbar.set_postfix({'cp31': np.mean(cp31), 'cp32': np.mean(cp32), 'cp33': np.mean(cp33)})\n",
    "        \n",
    "    print(f\"Check point 4: {time.time()-ts}\")\n",
    "    cap.release()\n",
    "    print(type(img))\n",
    "    print(img.shape)\n",
    "    print(img.dtype)\n",
    "    fpsT = int(fpsT)\n",
    "    print(type(fpsT))\n",
    "    torchvision.io.write_video(filename=o, video_array=img, fps=fpsT, video_codec='mpeg4') # notic: you should modeify the code in this function, line 140: frame.pict_type = \"NONE\" to frame.pict_type = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length =  116\n",
      "fps =  24.0\n",
      "Check point 1: 0.12832355499267578\n",
      "Check point 2: 0.17293572425842285\n",
      "Check point 3: 0.21001148223876953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [00:12<00:00,  9.03it/s, cp31=0.0332, cp32=0.0975, cp33=0.109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check point 4: 12.532719850540161\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([111, 720, 1280, 3])\n",
      "torch.uint8\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "videoConvert(\"new_test.mp4\",\"test_taylor.mp4\",3,0)\n",
    "# videoConvert(\"./brush.mp4\",\"./brush-taylor.mp4\",3,0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
